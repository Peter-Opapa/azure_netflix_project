# ğŸ¬ Netflix Data Engineering Pipeline on Azure

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Azure](https://img.shields.io/badge/Azure-0078D4?style=flat&logo=microsoft-azure&logoColor=white)](https://azure.microsoft.com/)
[![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=flat&logo=databricks&logoColor=white)](https://databricks.com/)
[![Delta Lake](https://img.shields.io/badge/Delta%20Lake-00ADD4?style=flat&logo=delta&logoColor=white)](https://delta.io/)

A comprehensive, production-ready data engineering solution for Netflix dataset analysis implementing modern data lakehouse architecture using Azure Data Factory, Azure Data Lake Storage Gen2, and Databricks with Delta Lake.

## ğŸ—ï¸ Architecture Overview

![Architecture Diagram](./docs/images/architecture_diagram.png)

This project implements a **medallion architecture** (Bronze â†’ Silver â†’ Gold) data lakehouse pattern with the following components:

### ğŸ”„ Data Flow Architecture
- **Data Ingestion**: Azure Data Factory (ADF) orchestrates automated data ingestion from GitHub repositories to ADLS Gen2
- **Bronze Layer**: Raw data landing zone in ADLS Gen2 with Databricks Autoloader for real-time streaming ingestion
- **Silver Layer**: Cleaned, validated, and transformed data using Delta Lake tables with data quality enforcement
- **Gold Layer**: Business-ready aggregated data optimized for analytics, reporting, and machine learning

![ADL Overview](./docs/images/ADL_overview.png)

## ğŸ“ Project Structure

```
ğŸ“¦ Netflix-Data-Pipeline/
â”œâ”€â”€ ğŸ“š docs/                           # Documentation and architecture diagrams
â”‚   â”œâ”€â”€ ğŸ–¼ï¸ images/                     # Architecture diagrams and screenshots
â”‚   â””â”€â”€ ğŸ”§ setup/                      # Setup and deployment guides
â”œâ”€â”€ ğŸ—ï¸ infrastructure/                 # Infrastructure as Code
â”‚   â”œâ”€â”€ ğŸ­ adf/                        # Azure Data Factory pipelines and configurations
â”‚   â”œâ”€â”€ âš¡ databricks/                 # Databricks workspace configurations
â”‚   â””â”€â”€ ğŸ’¾ storage/                    # ADLS Gen2 configurations
â”œâ”€â”€ ğŸ’» src/                           # Source code
â”‚   â”œâ”€â”€ ğŸ“Š databricks/                # Databricks notebooks and scripts
â”‚   â”‚   â”œâ”€â”€ ğŸ¥‰ bronze/                # Bronze layer transformations (raw data ingestion)
â”‚   â”‚   â”œâ”€â”€ ğŸ¥ˆ silver/                # Silver layer transformations (cleaned data)
â”‚   â”‚   â”œâ”€â”€ ğŸ¥‡ gold/                  # Gold layer transformations (business logic)
â”‚   â”‚   â””â”€â”€ ğŸ› ï¸ utils/                 # Utility functions and helpers
â”‚   â””â”€â”€ ğŸ­ data_factory/              # ADF pipeline definitions
â”œâ”€â”€ ğŸ“Š data/                          # Sample datasets and schemas
â”‚   â”œâ”€â”€ ğŸ“„ sample/                    # Sample Netflix datasets
â”‚   â””â”€â”€ ğŸ“‹ schemas/                   # Data schemas and documentation
â”œâ”€â”€ ğŸ§ª tests/                         # Unit and integration tests
â”œâ”€â”€ âš™ï¸ config/                        # Configuration files
â””â”€â”€ ğŸš€ deployment/                    # Deployment scripts and CI/CD
```

## âœ¨ Key Features

### ğŸ”„ **Real-time Data Processing**
- **Incremental Data Loading**: Databricks Autoloader for efficient streaming ingestion
- **Schema Evolution**: Automatic schema inference and evolution handling
- **Change Data Capture**: Only processes new/modified data for optimal performance

### ğŸ›¡ï¸ **Data Quality & Governance**
- **Delta Live Tables (DLT)**: Declarative pipelines with built-in data quality checks
- **Data Validation**: Comprehensive validation rules and data quality enforcement
- **ACID Transactions**: Delta Lake ensures data consistency and reliability
- **Data Lineage**: Full traceability from source to consumption

### ğŸ›ï¸ **Medallion Architecture**
- **Bronze Layer**: Raw data preservation with full audit trail
- **Silver Layer**: Cleaned, validated, and conformed data
- **Gold Layer**: Business-ready data marts for analytics and ML

### ğŸ“Š **Monitoring & Observability**
- **Pipeline Monitoring**: Built-in monitoring and alerting
- **Error Handling**: Robust error recovery and notification systems
- **Performance Metrics**: Comprehensive performance tracking and optimization

## ğŸ› ï¸ Technologies Used

<div align="center">

| Technology | Purpose | Version |
|------------|---------|---------|
| ![Azure Data Factory](https://img.shields.io/badge/Azure%20Data%20Factory-0078D4?style=for-the-badge&logo=microsoft-azure&logoColor=white) | Data orchestration and pipeline management | Latest |
| ![Azure Data Lake](https://img.shields.io/badge/Azure%20Data%20Lake-0078D4?style=for-the-badge&logo=microsoft-azure&logoColor=white) | Scalable data lake storage | Gen2 |
| ![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white) | Data processing and analytics platform | Runtime 13.3 LTS |
| ![Delta Lake](https://img.shields.io/badge/Delta%20Lake-00ADD4?style=for-the-badge&logo=delta&logoColor=white) | ACID transactions and data versioning | Latest |
| ![PySpark](https://img.shields.io/badge/PySpark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white) | Distributed data processing | 3.4+ |
| ![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white) | Programming language | 3.8+ |

</div>

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites

Before getting started, ensure you have:

- âœ… **Azure Subscription** with appropriate permissions
- âœ… **Azure CLI** installed and configured
- âœ… **Databricks Workspace** (Premium or Standard tier)
- âœ… **Azure Data Lake Storage Gen2** account
- âœ… **Azure Data Factory** instance
- âœ… **Python 3.8+** for local development

### âš¡ One-Click Setup

1. **Clone the Repository**
   ```bash
   git clone https://github.com/your-username/netflix-data-pipeline.git
   cd netflix-data-pipeline
   ```

2. **Configure Environment**
   ```bash
   # Copy configuration template
   cp config/config.env.template config/.env
   
   # Update with your Azure resource details
   nano config/.env
   ```

3. **Deploy Infrastructure**
   ```bash
   # Using PowerShell (Windows)
   .\deployment\deploy.ps1
   
   # Using Bash (Linux/Mac)
   chmod +x deployment/deploy.sh
   ./deployment/deploy.sh
   ```

4. **Follow Setup Guide**
   ```bash
   # Detailed instructions available in
   docs/setup/SETUP.md
   ```

## ğŸ”„ Data Pipeline Flow

![ADF Pipeline Overview](./docs/images/adf_overview.png)

### Phase 1: Data Ingestion ğŸ“¥
**Azure Data Factory** orchestrates the ingestion process:
- Copies Netflix CSV files from GitHub repositories
- Validates data integrity and completeness
- Stores raw data in ADLS Gen2 bronze layer

![ADF Pipeline Details](./docs/images/ADF_Pipeline.png)

### Phase 2: Bronze Layer Processing ğŸ¥‰
**Databricks Autoloader** handles streaming ingestion:
- Real-time data streaming from ADLS Gen2
- Automatic schema inference and evolution
- Creates Delta tables for ACID compliance

![Databricks Compute](./docs/images/compute_with_unity_catalog.png)

### Phase 3: Silver Layer Transformation ğŸ¥ˆ
**Data cleaning and validation**:
- Removes duplicates and handles null values
- Standardizes data formats and types
- Creates normalized lookup tables
- Applies data quality rules

### Phase 4: Gold Layer Analytics ğŸ¥‡
**Business-ready data preparation**:
- Aggregates data for reporting and analytics
- Applies business logic and calculations
- Optimizes for query performance
- Prepares features for machine learning

![Pipeline Runs](./docs/images/Pipeline-runs.png)

### ForEach Activity Implementation
![ForEach Activity](./docs/images/ForEachActivity.png)

The pipeline uses ForEach activities to process multiple Netflix dataset files in parallel, ensuring efficient and scalable data processing.

## ğŸ“Š Dataset Overview

### Netflix Content Dataset
This project processes comprehensive Netflix content data including:

| Dataset | Records | Description |
|---------|---------|-------------|
| **netflix_titles.csv** | ~8,800+ | Main titles dataset with metadata |
| **netflix_cast.csv** | ~25,000+ | Cast member relationships |
| **netflix_directors.csv** | ~4,000+ | Director relationships |
| **netflix_countries.csv** | ~7,500+ | Country/region associations |
| **netflix_categories.csv** | ~10,000+ | Genre and category mappings |

### Business Intelligence Insights
- ğŸ“ˆ **Content Growth Trends**: Year-over-year content addition patterns
- ğŸŒ **Global Distribution**: Geographic content distribution analysis
- ğŸ­ **Genre Analysis**: Popular genres and content categorization
- ğŸ‘¥ **Talent Networks**: Actor and director collaboration patterns
- ğŸ“º **Content Types**: Movies vs. TV Shows distribution analysis

## ğŸ”§ Development

### ğŸƒâ€â™‚ï¸ Running Locally

1. **Setup Development Environment**
   ```bash
   # Create virtual environment
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   
   # Install dependencies
   pip install -r requirements.txt
   ```

2. **Configure Databricks CLI**
   ```bash
   databricks configure --token
   # Enter your Databricks workspace URL and token
   ```

3. **Test Notebooks**
   ```bash
   # Upload notebooks to Databricks workspace
   databricks workspace import_dir src/databricks /Shared/netflix-pipeline
   ```

### ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/

# Run integration tests
pytest tests/integration/

# Generate test coverage report
pytest --cov=src tests/
```

## ğŸ“ˆ Performance Metrics

### Pipeline Performance
- **Data Ingestion**: ~500MB/min throughput
- **Bronze to Silver**: ~1GB/min processing rate
- **Silver to Gold**: ~2GB/min aggregation speed
- **End-to-End Latency**: < 30 minutes for full dataset

### Cost Optimization
- **Databricks Clusters**: Auto-scaling enabled (1-8 workers)
- **ADLS Gen2**: Hot tier for active data, Cool tier for archives
- **ADF**: Optimized for parallel processing and minimal execution time

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### ğŸ› Reporting Issues
- Use GitHub Issues for bug reports
- Include detailed steps to reproduce
- Provide environment details and error logs

### ğŸ’¡ Feature Requests
- Open a GitHub Issue with the "enhancement" label
- Describe the use case and expected behavior
- Consider submitting a Pull Request

### ğŸ”„ Pull Request Process
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“‹ Roadmap

### ğŸ¯ Upcoming Features
- [ ] **Real-time Streaming**: Kafka integration for live data feeds
- [ ] **ML Pipeline**: Automated feature engineering and model training
- [ ] **API Layer**: REST API for data access and query execution
- [ ] **Data Catalog**: Automated data discovery and documentation
- [ ] **Advanced Analytics**: Predictive analytics and recommendation engine

### ğŸ”® Future Enhancements
- [ ] **Multi-cloud Support**: AWS and GCP deployment options
- [ ] **Data Governance**: Advanced data lineage and privacy controls
- [ ] **Edge Computing**: IoT data integration capabilities
- [ ] **Advanced Visualizations**: Interactive dashboards and reports

## ğŸ“š Documentation

- ğŸ“– **[Setup Guide](docs/setup/SETUP.md)** - Detailed deployment instructions
- ğŸ—ï¸ **[Architecture Deep Dive](docs/architecture.md)** - Technical architecture details
- ğŸ”§ **[Configuration Reference](config/README.md)** - Configuration options
- ğŸ§ª **[Testing Guide](tests/README.md)** - Testing procedures and best practices
- ğŸ› **[Troubleshooting](docs/troubleshooting.md)** - Common issues and solutions

## ğŸ† Acknowledgments

- **Netflix** for providing the open dataset
- **Azure** for the robust cloud platform
- **Databricks** for the powerful analytics platform
- **Delta Lake** for reliable data lake storage
- **Community contributors** for their valuable input

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**â­ Star this repository if you found it helpful!**

[![GitHub stars](https://img.shields.io/github/stars/your-username/netflix-data-pipeline.svg?style=social&label=Star)](https://github.com/your-username/netflix-data-pipeline)
[![GitHub forks](https://img.shields.io/github/forks/your-username/netflix-data-pipeline.svg?style=social&label=Fork)](https://github.com/your-username/netflix-data-pipeline/fork)

**Built with â¤ï¸ for the Data Engineering Community**

</div>
